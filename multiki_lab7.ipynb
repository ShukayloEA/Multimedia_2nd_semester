{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # библиотека для работы с чиселками\n",
    "import os\n",
    "import pandas as pd # data processing, работа с CSV файлами\n",
    "import matplotlib.pyplot as plt # для графики\n",
    "import seaborn as sns # аналогично\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchmetrics.classification import BinaryJaccardIndex, BinaryF1Score, BinaryPrecision, BinaryRecall\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откроем описание датасета в формате CSV и посмотрим первые 5 строчек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class index</th>\n",
       "      <th>filepaths</th>\n",
       "      <th>labels</th>\n",
       "      <th>data set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train/Dark/dark (1).png</td>\n",
       "      <td>Dark</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>train/Dark/dark (10).png</td>\n",
       "      <td>Dark</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>train/Dark/dark (100).png</td>\n",
       "      <td>Dark</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>train/Dark/dark (101).png</td>\n",
       "      <td>Dark</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>train/Dark/dark (102).png</td>\n",
       "      <td>Dark</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class index                  filepaths labels data set\n",
       "0            0    train/Dark/dark (1).png   Dark    train\n",
       "1            0   train/Dark/dark (10).png   Dark    train\n",
       "2            0  train/Dark/dark (100).png   Dark    train\n",
       "3            0  train/Dark/dark (101).png   Dark    train\n",
       "4            0  train/Dark/dark (102).png   Dark    train"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Coffee_bean_dataset\\\\Coffee_Bean.csv', encoding='ISO-8859-1')\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_dir, label_dir, transform):\n",
    "    image_filenames = [f for f in os.listdir(image_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for filename in image_filenames:\n",
    "        # Загружаем изображение\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_np = np.array(img)\n",
    "        #img_tensor = transform(img)\n",
    "        #images.append(img_np)\n",
    "\n",
    "        # Загружаем соответствующую маску\n",
    "        label_path = os.path.join(label_dir, filename.replace(\".jpg\", \".png\"))\n",
    "        label = Image.open(label_path).convert(\"L\")\n",
    "        label = label.resize((224, 224), Image.NEAREST)\n",
    "        label_np = np.array(label)\n",
    "        \n",
    "        transformed = transform(image=img_np, mask=label_np)\n",
    "\n",
    "        labels.append(transformed['mask'].float())\n",
    "        images.append(transformed['image'].float())\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Трансформации\n",
    "transform = A.Compose([\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_train = \"C:\\\\Users\\\\Aila\\\\Уроки\\\\Multimedia_2nd_semester\\\\Coffee_bean_dataset\\\\segmentation_train\\\\images\"\n",
    "label_dir_train = \"C:\\\\Users\\\\Aila\\\\Уроки\\\\Multimedia_2nd_semester\\\\Coffee_bean_dataset\\\\segmentation_train\\\\labels\"\n",
    "image_dir_test = \"C:\\\\Users\\\\Aila\\\\Уроки\\\\Multimedia_2nd_semester\\\\Coffee_bean_dataset\\\\segmentation_test\\\\images\"\n",
    "label_dir_test = \"C:\\\\Users\\\\Aila\\\\Уроки\\\\Multimedia_2nd_semester\\\\Coffee_bean_dataset\\\\segmentation_test\\\\labels\"\n",
    "\n",
    "X_train, y_train = load_data(image_dir_train, label_dir_train, transform)\n",
    "X_test, y_test = load_data(image_dir_test, label_dir_test, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем данные в TensorDataset\n",
    "dataset = TensorDataset(torch.stack(X_train), torch.stack(y_train))\n",
    "dataset_test = TensorDataset(torch.stack(X_test), torch.stack(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим модели Unet и DeepLabV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модели и посмотрим на результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_name, dataset, num_of_epochs=5):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # Создаем DataLoader с батчами\n",
    "    train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # Обучение\n",
    "    for epoch in range(num_of_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            if y_batch.ndim == 4 and y_batch.shape[1] == 1:\n",
    "                y_batch = y_batch.squeeze(1)  # from (B, 1, H, W) → (B, H, W)\n",
    "            y_batch = (y_batch > 127).long()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            #print(1)\n",
    "            loss = criterion(outputs.squeeze(1), y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"{model_name}: Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def eval(model, model_name, test_dataset):\n",
    "    iou_metric = BinaryJaccardIndex(ignore_index=255).to(device)\n",
    "    dice_metric = BinaryF1Score(ignore_index=255).to(device)\n",
    "    precision_metric = BinaryPrecision(ignore_index=255).to(device)\n",
    "    recall_metric = BinaryRecall(ignore_index=255).to(device)\n",
    "    \n",
    "    iou_metric.reset()\n",
    "    dice_metric.reset()\n",
    "    precision_metric.reset()\n",
    "    recall_metric.reset()\n",
    "\n",
    "    # Создаем DataLoader с батчами\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    model.eval()\n",
    "    #y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            if y_batch.ndim == 4 and y_batch.shape[1] == 1:\n",
    "                y_batch = y_batch.squeeze(1)  # from (B, 1, H, W) → (B, H, W)\n",
    "            outputs = model(X_batch)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.3).squeeze(1).long()\n",
    "            y_batch = (y_batch > 127).long()\n",
    "            #preds = torch.sigmoid(outputs)  # Вероятности для класса объект\n",
    "            #preds = (preds > 0.5).squeeze(1).long()  # Преобразуем в бинарные метки (0 или 1)\n",
    "            #plt.imshow(preds[0].numpy(), cmap='gray')\n",
    "            #print(1)\n",
    "            #plt.imshow(preds[0].squeeze(0).numpy(), cmap='gray')\n",
    "            #print(2)\n",
    "            #plt.axis('off')\n",
    "            #plt.show()\n",
    "            # Обновляем метрики\n",
    "            iou_metric.update(preds, y_batch)\n",
    "            dice_metric.update(preds, y_batch)\n",
    "            precision_metric.update(preds, y_batch)\n",
    "            recall_metric.update(preds, y_batch)\n",
    "\n",
    "    print(f\"\\nEvaluation Metrics for {model_name}:\")\n",
    "    print(\"mIoU:\", iou_metric.compute().item())\n",
    "    print(\"Dice:\", dice_metric.compute().item())\n",
    "    print(\"Precision:\", precision_metric.compute().item())\n",
    "    print(\"Recall:\", recall_metric.compute().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics for Unet:\n",
      "mIoU: 0.9638083577156067\n",
      "Dice: 0.9815707206726074\n",
      "Precision: 0.9657078385353088\n",
      "Recall: 0.9979634284973145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unet = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=1\n",
    ").to(device)\n",
    "train(unet, \"Unet\", dataset=dataset, num_of_epochs=1)\n",
    "\n",
    "eval(unet, \"Unet\", test_dataset=dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeplab: Epoch 1, Loss: 0.1118\n",
      "Deeplab: Epoch 2, Loss: 0.0424\n",
      "Deeplab: Epoch 3, Loss: 0.0296\n",
      "\n",
      "Evaluation Metrics for Deeplab:\n",
      "mIoU: 0.9625186324119568\n",
      "Dice: 0.9809014201164246\n",
      "Precision: 0.9641741514205933\n",
      "Recall: 0.9982192516326904\n"
     ]
    }
   ],
   "source": [
    "deeplab = smp.DeepLabV3(\n",
    "    encoder_name=\"resnet34\",     \n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,             \n",
    "    classes=1                  \n",
    ").to(device)\n",
    "\n",
    "train(deeplab, \"Deeplab\", dataset=dataset, num_of_epochs=3)\n",
    "eval(deeplab, \"Deeplab\", test_dataset=dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet, \"unet_full.pth\")\n",
    "torch.save(deeplab, \"deeplabv3_full.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод:\n",
    "1. Метрики для Unet:\n",
    "Модель Unet показала высокие значения по всем ключевым метрикам, что свидетельствует о стабильной и точной сегментации с минимальными пропущенными пикселями объектов. Высокий Recall говорит о том, что модель почти не упускает истинные положительные объекты, а высокие Precision и Dice подтверждают, что ложноположительных предсказаний почти нет.\n",
    "2. Метрики для DeepLabv3:\n",
    "DeepLabv3 также показал почти идентичные показатели. Это указывает на сопоставимую производительность с Unet, но немного ниже precision, что может говорить о чуть большей склонности к ложноположительным предсказаниям. Однако разница крайне мала — обе модели демонстрируют высокую обобщающую способность и уверенно решают задачу.\n",
    "\n",
    "Обе архитектуры успешно справляются с поставленной задачей и пригодны для практического применения без необходимости глубокой дополнительной настройки. Модели не переобучились, предсказания уверенные и согласованные по метрикам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для улучшения бейзлайна модели в задачи семантической сегментации предлагаю следующие решения:\n",
    "\n",
    "Провести аугментацию тренировочных данных: использовать нормализацию, повороты и изменить яркость и контраст изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аугментация тренировочного датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aug_transform = A.Compose([\n",
    "    A.Rotate(limit=15, p=1.0),  # случайный поворот до 15 градусов\n",
    "    A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=70, val_shift_limit=40, p=1.0),  # изменение оттенка, насыщенности, яркости\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "X_train_new, y_train_new = load_data(image_dir_train, label_dir_train, aug_transform)\n",
    "X_test_new, y_test_new = load_data(image_dir_test, label_dir_test, aug_transform)\n",
    "\n",
    "dataset_new = TensorDataset(torch.stack(X_train_new), torch.stack(y_train_new))\n",
    "dataset_test_new = TensorDataset(torch.stack(X_test_new), torch.stack(y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet: Epoch 1, Loss: 0.2898\n",
      "Unet: Epoch 2, Loss: 0.1006\n",
      "Unet: Epoch 3, Loss: 0.0615\n",
      "\n",
      "Evaluation Metrics for Unet:\n",
      "mIoU: 0.9717341065406799\n",
      "Dice: 0.9856644868850708\n",
      "Precision: 0.9758588671684265\n",
      "Recall: 0.9956691265106201\n"
     ]
    }
   ],
   "source": [
    "unet_new = smp.Unet(\n",
    "    encoder_name=\"resnet34\",     \n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,             \n",
    "    classes=1                  \n",
    ").to(device)\n",
    "\n",
    "train(unet_new, \"Unet\", dataset=dataset_new, num_of_epochs=3)\n",
    "eval(unet_new, \"Unet\", test_dataset=dataset_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeplab: Epoch 1, Loss: 0.1342\n",
      "Deeplab: Epoch 2, Loss: 0.0580\n",
      "Deeplab: Epoch 3, Loss: 0.0390\n",
      "\n",
      "Evaluation Metrics for Deeplab:\n",
      "mIoU: 0.9629967212677002\n",
      "Dice: 0.9811496138572693\n",
      "Precision: 0.9659270644187927\n",
      "Recall: 0.996859610080719\n"
     ]
    }
   ],
   "source": [
    "deeplab_new = smp.DeepLabV3(\n",
    "    encoder_name=\"resnet34\",     \n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,             \n",
    "    classes=1                  \n",
    ").to(device)\n",
    "\n",
    "train(deeplab_new, \"Deeplab\", dataset=dataset_new, num_of_epochs=3)\n",
    "eval(deeplab_new, \"Deeplab\", test_dataset=dataset_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet_new, \"unet_new_full.pth\")\n",
    "torch.save(deeplab_new, \"deeplabv3_new_full.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод:\n",
    "\n",
    "Unet после улучшения демонстрирует ещё более высокое качество сегментации. Рост всех метрик — особенно Dice (вырос с 0.98 до 0.99) и mIoU (вырос с 0.96 до 0.97) — говорит о том, что модель стала лучше захватывать границы объектов, снижая количество как пропущенных, так и ошибочно предсказанных пикселей.\n",
    "\n",
    "DeepLabv3 по-прежнему показывает высокое и стабильное качество, остаётся сильной, особенно по recall (0.99), но немного уступает улучшенному Unet. Это говорит о том, что без дополнительной настройки DeepLabv3 сохраняет достойные результаты, но менее чувствителен к мелким улучшениям по сравнению с Unet.\n",
    "\n",
    "Таким образом, улучшения в виде аугментаций, подбора гиперпараметров и архитектурных изменений оказали положительное влияние на обе модели.\n",
    "После доработки Unet стал заметно лучше, выйдя вперёд по всем метрикам. DeepLabv3 по-прежнему силён, особенно в задачах с разными масштабами объектов, но Unet после улучшений — более эффективное решение для данной задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv => BN => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # Pad x1 to match x2 if needed\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_conv = DoubleConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "\n",
    "        self.up1 = Up(1024 + 512, 512, bilinear)\n",
    "        self.up2 = Up(512 + 256, 256, bilinear)\n",
    "        self.up3 = Up(256 + 128, 128, bilinear)\n",
    "        self.up4 = Up(128 + 64, 64, bilinear)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.in_conv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        return self.out_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, rates=[6, 12, 18]):\n",
    "        super(ASPP, self).__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        self.blocks.append(nn.Sequential(  # 1x1 conv\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "        for rate in rates:\n",
    "            self.blocks.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=rate, dilation=rate, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "        self.global_pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(out_channels * (len(rates) + 2), out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[2:]\n",
    "        res = [block(x) for block in self.blocks]\n",
    "        gp = self.global_pool(x)\n",
    "        gp = F.interpolate(gp, size=size, mode='bilinear', align_corners=False)\n",
    "        res.append(gp)\n",
    "        x = torch.cat(res, dim=1)\n",
    "        return self.project(x)\n",
    "\n",
    "class DeepLabV3(nn.Module):\n",
    "    def __init__(self, num_classes=1, backbone='resnet50', pretrained=True):\n",
    "        super(DeepLabV3, self).__init__()\n",
    "        resnet = resnet50(pretrained=pretrained)\n",
    "\n",
    "        self.backbone = nn.Sequential(\n",
    "            resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool,\n",
    "            resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4  # output stride 32\n",
    "        )\n",
    "        self.aspp = ASPP(in_channels=2048, out_channels=256)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, num_classes, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_size = x.shape[2:]\n",
    "        features = self.backbone(x)\n",
    "        x = self.aspp(features)\n",
    "        x = self.classifier(x)\n",
    "        return F.interpolate(x, size=input_size, mode='bilinear', align_corners=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим работу имплементированного алгоритма на обычном датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet: Epoch 1, Loss: 0.2447\n",
      "Unet: Epoch 2, Loss: 0.1845\n",
      "Unet: Epoch 3, Loss: 0.1563\n",
      "\n",
      "Evaluation Metrics for Unet:\n",
      "mIoU: 0.9643996953964233\n",
      "Dice: 0.9818772673606873\n",
      "Precision: 0.9661650061607361\n",
      "Recall: 0.998108983039856\n"
     ]
    }
   ],
   "source": [
    "my_unet = UNet(in_channels=3, out_channels=1).to(device)\n",
    "train(my_unet, \"Unet\", dataset=dataset, num_of_epochs=3)\n",
    "eval(my_unet, \"Unet\", test_dataset=dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Aila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\Aila/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:16<00:00, 6.40MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeplab: Epoch 1, Loss: 0.1742\n",
      "Deeplab: Epoch 2, Loss: 0.0948\n",
      "Deeplab: Epoch 3, Loss: 0.0759\n",
      "\n",
      "Evaluation Metrics for Deeplab:\n",
      "mIoU: 0.8728695511817932\n",
      "Dice: 0.9321199655532837\n",
      "Precision: 0.8731996417045593\n",
      "Recall: 0.9995671510696411\n"
     ]
    }
   ],
   "source": [
    "my_deeplabv3 = DeepLabV3(num_classes=1).to(device)\n",
    "train(my_deeplabv3, \"Deeplab\", dataset=dataset, num_of_epochs=3)\n",
    "eval(my_deeplabv3 , \"Deeplab\", test_dataset=dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь проверим на улучшенном бейзлайне:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet: Epoch 1, Loss: 0.2978\n",
      "Unet: Epoch 2, Loss: 0.2246\n",
      "Unet: Epoch 3, Loss: 0.1888\n",
      "\n",
      "Evaluation Metrics for Unet:\n",
      "mIoU: 0.9507742524147034\n",
      "Dice: 0.9747660160064697\n",
      "Precision: 0.9530373215675354\n",
      "Recall: 0.997508704662323\n"
     ]
    }
   ],
   "source": [
    "my_unet_new = UNet(in_channels=3, out_channels=1).to(device)\n",
    "train(my_unet_new, \"Unet\", dataset=dataset_new, num_of_epochs=3)\n",
    "eval(my_unet_new, \"Unet\", test_dataset=dataset_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Aila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeplab: Epoch 1, Loss: 0.2133\n",
      "Deeplab: Epoch 2, Loss: 0.1190\n",
      "Deeplab: Epoch 3, Loss: 0.0934\n",
      "\n",
      "Evaluation Metrics for Deeplab:\n",
      "mIoU: 0.8866561055183411\n",
      "Dice: 0.9399234056472778\n",
      "Precision: 0.8876857757568359\n",
      "Recall: 0.9986934661865234\n"
     ]
    }
   ],
   "source": [
    "my_deeplabv3_new = DeepLabV3(num_classes=1).to(device)\n",
    "train(my_deeplabv3_new, \"Deeplab\", dataset=dataset_new, num_of_epochs=3)\n",
    "eval(my_deeplabv3_new , \"Deeplab\", test_dataset=dataset_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сравнение собственной реализации U-Net и DeepLabV3 до улучшения бейзлайна:\n",
    "До улучшения бейзлайна U-Net показывает уверенно высокие результаты по всем метрикам: mIoU — 0.964, Dice — 0.981, Precision — 0.966, Recall — 0.998. Это говорит о высокой согласованности предсказаний модели с истинной разметкой и хорошем обобщении. В то же время DeepLabV3 заметно отстаёт: хотя Recall у него также близок к 1.0 (что означает, что модель почти не пропускает объекты), Precision — всего 0.873, что указывает на большое количество ложноположительных срабатываний. mIoU — 0.873 и Dice — 0.932 подтверждают, что качество сегментации хуже, чем у U-Net.\n",
    "\n",
    "##### Сравнение собственной реализации U-Net и DeepLabV3 после улучшения бейзлайна:\n",
    "После доработки бейзлайна качество обеих моделей выросло, но в разной степени. U-Net по-прежнему лидирует: mIoU = 0.950, Dice = 0.975, Precision = 0.953 и Recall = 0.998. Это говорит о сбалансированном и точном предсказании всех классов. DeepLabV3 также немного улучшился, но остаётся менее точным: Dice — 0.940, Precision — 0.887, mIoU — 0.887. Основная проблема по-прежнему заключается в сравнительно низкой точности (Precision), несмотря на высокий Recall, что может свидетельствовать о переизбыточных предсказаниях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод:\n",
    "Собственная реализация U-Net демонстрирует наилучшие метрики как до, так и после улучшения бейзлайна, уверенно опережая DeepLabV3. Последний улучшился, но не догнал U-Net по качеству. Это может быть связано с тем, что U-Net более эффективно использует пространственную информацию в задачах бинарной сегментации, в то время как DeepLabV3 требует более тонкой настройки или сложной архитектурной адаптации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
