# Multimedia_2nd_semester
Студент: Шукайло Екатерина Алексеевна
Группа: М8О-401Б-21

### Выбор начальных условий:
Набор данных: я выбрала небольшой датасет Coffee bean Dataset Resized (224 X 224), который содержит изображения обжаренных кофейных зерен разной степени обжарки. Всего их 4: класс Dark с индексом класса 0, класс Green с индексом 1, класс Light с инексом 2 и класс Medium с индексом класса 3. Всего имеется 4800 фотографий, под каждой степенью находится 1200 фотографий.


Данные из этого датасета подходят для задач классификации, сегментации и детекции по нескольким причинам:
- Наличие 4 классов обеспечивает достаточную вариативность данных при сохранении понятной интерпретируемости результатов, что позволяет объективно сравнить поведение различных архитектур без избыточной нагрузки на вычислительные ресурсы;
- Датасет содержит сбалансированные по числу примеров категории, что снижает риск смещения модели в сторону преобладающего класса и обеспечивает более честную оценку точности;
- Классы представлены на основе визуально различимых объектов, что делает задачу пригодной для тестирования архитектур компьютерного зрения;
- Используемые изображения имеют различное освещение, ракурсы и фон, приближая условия к реальным задачам компьютерного зрения и обеспечивая обобщающую способность моделей.

### Метрики качества:
В качестве метрик я использовала accuracy, precision, recall и  F1-score для задачи классификации, mIoU, dice, precision, recall для задачи семантической сегментации и precision, recall, mAP@0.5 и mAP@0.5:0.95 для задачи детекции.

## Подведение итогов:

### Лабораторная работа №6
Посмотрим метрики качества на тестовом наборе данных для каждой из реализаций алгоритмов. Первое число метрики показыает значение на обычном бейзлайне, второе - на улучшенном:
| Алгоритм | Accuracy | Precision | Recall | F1 score |
| --- | --- | --- | --- | --- |
| ResNet18 | 0.515/0.89 | 0.807/0.90 | 0.515/0.89 | 0.394/0.89 |
| --- | --- | --- | --- | --- |
| ViT-B_16 | 0.975/0.97 | 0.975/0.97 | 0.975/0.97 | 0.975/0.97 |
| --- | --- | --- | --- | --- |
| My ResNet18 | 0.6375/0.54 | 0.54/0.625 | 0.6375/0.54 | 0.55/0.52 |
| --- | --- | --- | --- | --- |
| My ViT-B_16 | 0.25/0.25 | 0.0625/0.0625 | 0.25/0.25 | 0.1/0.1 |

После анализа представленных результатов видно, что улучшение бейзлайна положительно повлияло на большинство моделей, но степень этого влияния заметно различается между архитектурами.

Наибольший прирост показала базовая реализация ResNet18, чьи метрики выросли практически по всем направлениям: Accuracy с 0.515 до 0.89, F1-score с 0.394 до 0.89. Это указывает на то, что изначально модель плохо справлялась с задачей, но после оптимизаций смогла эффективно обучиться и показать отличный результат.

ViT-B_16 показал стабильно высокие значения как до, так и после улучшений (F1-score = 0.975/0.97), что говорит о высокой устойчивости модели к качеству бейзлайна. Лёгкое снижение после улучшения может быть связано с нюансами обучения или переобучением на обновлённом датасете.

Собственная реализация ResNet18 после улучшения, напротив, показала небольшое снижение метрик (F1-score: с 0.55 до 0.52), что может говорить о чувствительности архитектуры к изменениям. Однако даже базовые показатели (до улучшения) были выше, чем у предобученной ResNet18, что подчёркивает потенциал ручной настройки модели.

Собственная ViT-модель (My ViT-B_16) осталась на крайне низких показателях во всех сценариях, демонстрируя F1-score = 0.1. Это указывает на неспособность обучаться без предобученных весов и правильной настройки архитектуры. Модель, вероятно, просто предсказывает один и тот же класс.

###### Заключение:
Улучшение бейзлайна дало максимальный эффект для предобученного ResNet18, сделав его конкурентоспособным по всем метрикам. Для собственных реализаций результат неоднозначен: собственная имплементация ResNet уже имела неплохую обобщающую способность, а имплементация ViT требует радикальной доработки (дообучение, архитектурные правки). Лидером по стабильности и абсолютным метрикам остаётся ViT-B_16, показывая, что мощные трансформеры при правильной инициализации практически не зависят от улучшений в бейзлайне.


### Лабораторная работа №7
Посмотрим метрики качества на тестовом наборе данных для каждой из реализаций алгоритмов. Первое число метрики показыает значение на обычном бейзлайне, второе - на улучшенном:
| Алгоритм | mIoU | Dice | Precision | Recall |
| --- | --- | --- | --- | --- |
| Unet | 0.96/0.97 | 0.98/0.985 | 0.97/0.975 | 0.998/0.996 |
| --- | --- | --- | --- | --- |
| DeepLabV3 | 0.9625/0.9629 | 0.981/0.98 | 0.964/0.97 | 0.998/0.997 |
| --- | --- | --- | --- | --- |
| My Unet | 0.96/0.95 | 0.98/0.97 | 0.97/0.95 | 0.99/0.99 |
| --- | --- | --- | --- | --- |
| My DeepLabV3 | 0.87/0.88 | 0.93/0.94 | 0.87/0.89 | 0.99/0.99 |

Сегментационные модели продемонстрировали стабильно высокие показатели, однако влияние улучшения бейзлайна проявилось по-разному.

Предобученный Unet показал незначительное, но стабильное улучшение по всем метрикам: mIoU вырос с 0.96 до 0.97, Dice — с 0.98 до 0.985. Несмотря на небольшое снижение Recall (с 0.998 до 0.996), общая сбалансированность и точность модели улучшились. Это указывает на хорошую восприимчивость модели к оптимизациям при уже высоком базовом уровне.

DeepLabV3 также продемонстрировал стабильные метрики до и после изменений. Улучшения были минимальными: mIoU увеличилось на 0.0004, Dice даже слегка снизилось (0.981 → 0.980), а Recall упал с 0.998 до 0.997. Это говорит о том, что DeepLabV3 изначально был хорошо адаптирован к задаче и почти не выигрывает от улучшений.

Собственная реализация Unet изначально показывала отличные метрики, близкие к предобученному аналогу. После улучшения бейзлайна, наоборот, наблюдается незначительное снижение: mIoU с 0.96 до 0.95, Dice — с 0.98 до 0.97, а Precision — с 0.97 до 0.95. Тем не менее, Recall остался неизменным на уровне 0.99. Это может свидетельствовать о небольшой переадаптации модели к новому распределению данных и потребности в дополнительной регулировке.

Собственная реализация DeepLabV3, напротив, наиболее выиграла от улучшения: mIoU вырос с 0.87 до 0.88, Dice — с 0.93 до 0.94, Precision — с 0.87 до 0.89. Несмотря на то, что абсолютные значения всё ещё уступают другим моделям, прирост показывает, что модель успешно реагирует на улучшения пайплайна и обладает потенциалом при дальнейшем развитии.

###### Заключение:
Наиболее стабильными оказались предобученные Unet и DeepLabV3, чьи метрики были высокими изначально и почти не изменились. Собственный Unet немного просел после улучшений, что может говорить о чувствительности к параметрам обучения. Имплементированный DeepLabV3, несмотря на отставание в абсолютных значениях, показал наибольший рост, демонстрируя хорошую обучаемость и адаптацию. В совокупности это говорит о высоком потенциале кастомных моделей при наличии точной настройки и грамотного бейзлайна.

### Лабораторная работа №8
Посмотрим метрики качества на тестовом наборе данных для каждой из реализаций алгоритмов. Первое число метрики показыает значение на обычном бейзлайне, второе - на улучшенном:
| Алгоритм | Precision | Recall | mAP50 | mAP50-95 |
| --- | --- | --- | --- | --- |
| YOLO11n | 0.93/0.74 | 0.69/0.91 | 0.93/0.94 | 0.91/0.81 |
| --- | --- | --- | --- | --- |
| YOLO11s | 0.936/0.98 | 0.94/0.98 | 0.97/0.99 | 0.91/0.86 |
| --- | --- | --- | --- | --- |
| My YOLO11n | 0.75/0.66 | 0.95/0.11 | 0.75/0.66 | 0.1/0.12 |
| --- | --- | --- | --- | --- |
| My YOLO11s | 0.78/0.89 | 0.90/0.18 | 0.78/0.89 | 0.23/0.18 |

Предобученные модели YOLO11n и YOLO11s показали хорошие результаты в обоих случаях, однако эффект улучшения особенно заметен у YOLO11s. У этой модели Precision, Recall и mAP50 выросли до почти максимальных значений (0.98, 0.98 и 0.99 соответственно), что указывает на высокую точность и полноту при детекции. YOLO11n, напротив, после улучшений несколько снизила Precision (с 0.93 до 0.74), но резко повысила Recall (с 0.69 до 0.91), что может свидетельствовать о том, что модель стала менее избирательной, но лучше находить все объекты.

Собственные реализации (My YOLO11n и My YOLO11s) продемонстрировали нестабильное поведение. У My YOLO11n Recall упал с 0.95 до 0.11 после улучшений, что говорит о критическом снижении способности находить объекты. Также mAP50 и mAP50-95 остались на низком уровне. У My YOLO11s, несмотря на рост Precision с 0.78 до 0.89, Recall также упал (с 0.90 до 0.18), а mAP50-95 снизился с 0.23 до 0.18. Это указывает на возможные проблемы с переобучением или несовместимость архитектуры с обновленным пайплайном обучения.

###### Заключение:
YOLO11s показал наилучшие и самые сбалансированные результаты как до, так и после улучшения бейзлайна, что делает его оптимальным выбором для задачи детекции. YOLO11n стал лучше выявлять объекты, но в ущерб точности. Собственные реализации моделей YOLO требуют доработки — особенно в части Recall, что критично для задач, где важно находить все объекты на изображении. Улучшение бейзлайна в их случае не обеспечило ожидаемого прироста, а местами даже ухудшило результат, что указывает на необходимость анализа архитектурных ограничений или параметров обучения.