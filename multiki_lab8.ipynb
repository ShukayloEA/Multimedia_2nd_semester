{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # библиотека для работы с чиселками\n",
    "import os\n",
    "import pandas as pd # data processing, работа с CSV файлами\n",
    "import matplotlib.pyplot as plt # для графики\n",
    "import seaborn as sns # аналогично\n",
    "from PIL import Image\n",
    "import torch\n",
    "import shutil\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откроем описание датасета в формате CSV и посмотрим первые 5 строчек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "      <th>bb1</th>\n",
       "      <th>bb2</th>\n",
       "      <th>bb3</th>\n",
       "      <th>bb4</th>\n",
       "      <th>data set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coffee_bean_dataset/train/Dark/dark (1).png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.504464</td>\n",
       "      <td>0.544643</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coffee_bean_dataset/train/Dark/dark (10).png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.466518</td>\n",
       "      <td>0.522321</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coffee_bean_dataset/train/Dark/dark (100).png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.491071</td>\n",
       "      <td>0.511161</td>\n",
       "      <td>0.526786</td>\n",
       "      <td>0.566964</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coffee_bean_dataset/train/Dark/dark (101).png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.533482</td>\n",
       "      <td>0.598214</td>\n",
       "      <td>0.566964</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coffee_bean_dataset/train/Dark/dark (102).png</td>\n",
       "      <td>0</td>\n",
       "      <td>0.486607</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        filename  class       bb1       bb2  \\\n",
       "0    Coffee_bean_dataset/train/Dark/dark (1).png      0  0.504464  0.544643   \n",
       "1   Coffee_bean_dataset/train/Dark/dark (10).png      0  0.466518  0.522321   \n",
       "2  Coffee_bean_dataset/train/Dark/dark (100).png      0  0.491071  0.511161   \n",
       "3  Coffee_bean_dataset/train/Dark/dark (101).png      0  0.482143  0.533482   \n",
       "4  Coffee_bean_dataset/train/Dark/dark (102).png      0  0.486607  0.513393   \n",
       "\n",
       "        bb3       bb4 data set  \n",
       "0  0.500000  0.696429    train  \n",
       "1  0.656250  0.500000    train  \n",
       "2  0.526786  0.566964    train  \n",
       "3  0.598214  0.566964    train  \n",
       "4  0.589286  0.580357    train  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Coffee_bean_dataset\\\\Coffee_bean_detections.csv', sep=\";\")\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем данные к YOLO формату для обучения и тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Базовая директория для нового датасета\n",
    "output_dir = 'Coffee_bean_dataset'\n",
    "image_out = os.path.join(output_dir, 'images')\n",
    "label_out = os.path.join(output_dir, 'labels')\n",
    "\n",
    "# Создаем нужные папки\n",
    "for split in ['train', 'test']:\n",
    "    os.makedirs(os.path.join(image_out, split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(label_out, split), exist_ok=True)\n",
    "\n",
    "# Преобразуем все строки\n",
    "for idx, row in dataset.iterrows():\n",
    "    image_path = row['filename']\n",
    "    class_id = int(row['class'])\n",
    "    split = row['data set']  # 'train' или 'test'\n",
    "\n",
    "    # Координаты bbox\n",
    "    x_center = float(row['bb1'])\n",
    "    y_center = float(row['bb2'])\n",
    "    width = float(row['bb3'])\n",
    "    height = float(row['bb4'])\n",
    "\n",
    "    # Имя файла без пути и расширения\n",
    "    filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "    # Сохраняем изображение\n",
    "    dst_img_path = os.path.join(image_out, split, f\"{filename}.jpg\")\n",
    "    shutil.copy(image_path, dst_img_path)\n",
    "\n",
    "    # Сохраняем аннотацию в YOLO формате\n",
    "    label_path = os.path.join(label_out, split, f\"{filename}.txt\")\n",
    "    with open(label_path, 'w') as f:\n",
    "        f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим модели YOLOv11n и YOLOv11s. Эти модели выбирались по принципу самая маленькая (YOLOv11n) и самая большая, однако на моем ноутбуке не смогли запуститься ни YOLOv11x, ни YOLOv11l, ни YOLOv11m, только YOLOv11s, поэтому анализ проводился именно с этими иоделями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модели и посмотрим на результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.40  Python-3.10.11 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,932 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\Coffee_bean_dataset\\YOLO_test\\labels.cache... 400 images, 0 backgrounds, 0 corrupt: 100%|██████████| 400/400 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:28<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400       0.93      0.697      0.936      0.913\n",
      "                  dark        100        100      0.852       0.99      0.988      0.961\n",
      "                 green        100        100          1       0.19      0.851      0.819\n",
      "                 light        100        100      0.986      0.702      0.968      0.948\n",
      "                medium        100        100      0.883      0.905      0.937      0.923\n",
      "Speed: 1.1ms preprocess, 60.1ms inference, 0.0ms loss, 4.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val4\u001b[0m\n",
      "metrics/precision(B): 0.9301\n",
      "metrics/recall(B): 0.6968\n",
      "metrics/mAP50(B): 0.9359\n",
      "metrics/mAP50-95(B): 0.9129\n",
      "fitness: 0.9152\n"
     ]
    }
   ],
   "source": [
    "yolov11n = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "yolov11n.train(\n",
    "    data='C:\\\\Users\\\\Aila\\\\Уроки\\\\Multimedia_2nd_semester\\\\data.yaml',\n",
    "    epochs=1,\n",
    "    imgsz=640,\n",
    "    device=device,\n",
    "    batch=16\n",
    ")\n",
    "\n",
    "metrics11n = yolov11n.val()\n",
    "for k, v in metrics11n.results_dict.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.128 available  Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=C:\\Users\\Aila\\\\Multimedia_2nd_semester\\runs\\detect\\train15\\weights\\best.pt, data=C:\\Users\\Aila\\\\Multimedia_2nd_semester\\data.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train18, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train18\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1    820956  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
      "YOLO11s summary: 319 layers, 9,429,340 parameters, 9,429,324 gradients, 21.6 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/shukayloea/general/4e25f0a5ccd948b2b527048f75f427b5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mWARNING  TensorBoard not initialized correctly, not logging this run. runs\\detect\\train18 is not a directory\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\Coffee_bean_dataset\\YOLO_train\\labels.cache... 1200 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1200/1200 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\Coffee_bean_dataset\\YOLO_test\\labels.cache... 400 images, 0 backgrounds, 0 corrupt: 100%|██████████| 400/400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train18\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train18\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/3         0G     0.4164     0.4648     0.9816         41        640: 100%|██████████| 75/75 [19:00<00:00, 15.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [01:15<00:00,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400      0.918      0.959      0.974      0.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/3         0G     0.4382     0.5084          1         39        640: 100%|██████████| 75/75 [10:22<00:00,  8.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [01:15<00:00,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400       0.75       0.84      0.922       0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/3         0G      0.441     0.4968     0.9977         41        640: 100%|██████████| 75/75 [10:36<00:00,  8.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [01:15<00:00,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400      0.874      0.882      0.907      0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 epochs completed in 0.731 hours.\n",
      "Optimizer stripped from runs\\detect\\train18\\weights\\last.pt, 19.2MB\n",
      "Optimizer stripped from runs\\detect\\train18\\weights\\best.pt, 19.2MB\n",
      "\n",
      "Validating runs\\detect\\train18\\weights\\best.pt...\n",
      "Ultralytics 8.3.40  Python-3.10.11 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "YOLO11s summary (fused): 238 layers, 9,414,348 parameters, 0 gradients, 21.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [01:05<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400      0.936      0.939      0.974      0.907\n",
      "                  dark        100        100      0.797      0.823      0.915      0.773\n",
      "                 green        100        100          1       0.99      0.995      0.949\n",
      "                 light        100        100      0.978          1      0.995      0.955\n",
      "                medium        100        100      0.969      0.943      0.991      0.949\n",
      "Speed: 1.3ms preprocess, 155.0ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train18\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : coffee_caterpillar_491\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/shukayloea/general/4e25f0a5ccd948b2b527048f75f427b5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg0 [4]               : (0.0004111111111111111, 0.0005546111111111112)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg1 [4]               : (0.0004111111111111111, 0.0005546111111111112)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg2 [4]               : (0.0004111111111111111, 0.0005546111111111112)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP50(B) [4]     : (0.90709, 0.9740876023129399)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP50-95(B) [4]  : (0.87027, 0.9066462326543989)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/precision(B) [4] : (0.75026, 0.9360484780265208)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/recall(B) [4]    : (0.84043, 0.95944)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/GFLOPs             : 21.555\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/parameters         : 9429340\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/speed_PyTorch(ms)  : 164.688\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/box_loss [3]       : (0.41644, 0.44101)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/cls_loss [3]       : (0.46479, 0.50841)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/dfl_loss [3]       : (0.98162, 1.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/box_loss [3]         : (0.24941, 0.35096)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/cls_loss [3]         : (0.42993, 1.13221)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/dfl_loss [3]         : (0.85006, 0.92066)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Created from                 : ultralytics\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_batch_logging_interval  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     log_confusion_matrix_on_eval : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     log_image_predictions        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_image_predictions        : 100\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     agnostic_nms    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     amp             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     augment         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     auto_augment    : randaugment\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch           : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     bgr             : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     box             : 7.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cache           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cfg             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     classes         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     close_mosaic    : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cls             : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conf            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     copy_paste      : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     copy_paste_mode : flip\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cos_lr          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     crop_fraction   : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data            : C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\data.yaml\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     degrees         : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     deterministic   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     device          : cpu\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dfl             : 1.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dnn             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout         : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dynamic         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     embed           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs          : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     erasing         : 0.4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exist_ok        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fliplr          : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flipud          : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     format          : torchscript\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fraction        : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     freeze          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     half            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_h           : 0.015\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_s           : 0.7\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_v           : 0.4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     imgsz           : 640\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     int8            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iou             : 0.7\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     keras           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kobj            : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     line_width      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr0             : 0.01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lrf             : 0.01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mask_ratio      : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_det         : 300\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mixup           : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mode            : train\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model           : C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\runs\\detect\\train15\\weights\\best.pt\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     momentum        : 0.937\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mosaic          : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     multi_scale     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name            : train18\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nbs             : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nms             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     opset           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimize        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer       : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     overlap_mask    : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     patience        : 100\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     perspective     : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     plots           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pose            : 12.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pretrained      : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     profile         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     project         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rect            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resume          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     retina_masks    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save            : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_conf       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_crop       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_dir        : runs\\detect\\train18\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_frames     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_hybrid     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_json       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_period     : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_txt        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     scale           : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seed            : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     shear           : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_boxes      : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_conf       : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_labels     : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     simplify        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     single_cls      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     split           : val\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stream_buffer   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     task            : detect\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     tracker         : botsort.yaml\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     translate       : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     verbose         : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     vid_stride      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     visualize       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_bias_lr  : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_epochs   : 3.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_momentum : 0.8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay    : 0.0005\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workers         : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workspace       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     confusion-matrix    : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images              : 17\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element       : 1 (18.28 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 3 metrics, params and output messages\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 4 file(s), remaining 17.68 MB/19.46 MB\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 10.52 MB/18.28 MB, Throughput 484.06 KB/s, ETA ~23s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 3.44 MB/18.28 MB, Throughput 478.19 KB/s, ETA ~8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.40  Python-3.10.11 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "YOLO11s summary (fused): 238 layers, 9,414,348 parameters, 0 gradients, 21.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\Coffee_bean_dataset\\YOLO_test\\labels.cache... 400 images, 0 backgrounds, 0 corrupt: 100%|██████████| 400/400 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [01:02<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400      0.936      0.939      0.974      0.907\n",
      "                  dark        100        100      0.797      0.823      0.915      0.773\n",
      "                 green        100        100          1       0.99      0.995      0.949\n",
      "                 light        100        100      0.978          1      0.995      0.955\n",
      "                medium        100        100      0.969      0.943      0.991      0.949\n",
      "Speed: 1.0ms preprocess, 151.2ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train182\u001b[0m\n",
      "metrics/precision(B): 0.9360\n",
      "metrics/recall(B): 0.9391\n",
      "metrics/mAP50(B): 0.9741\n",
      "metrics/mAP50-95(B): 0.9066\n",
      "fitness: 0.9134\n"
     ]
    }
   ],
   "source": [
    "yolov11s = YOLO(\"yolo11s.pt\")\n",
    "\n",
    "yolov11s.train(\n",
    "    data='C:\\\\Users\\\\Aila\\\\Уроки\\\\Multimedia_2nd_semester\\\\data.yaml',\n",
    "    epochs=3,\n",
    "    imgsz=640,\n",
    "    device=device,\n",
    "    batch=16\n",
    ")\n",
    "\n",
    "metrics11s = yolov11s.val()\n",
    "for k, v in metrics11s.results_dict.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод:\n",
    "1. Метрики для YOLOv11s:\n",
    "YOLOv11s показала высокую стабильность: точность 0.9360, полнота 0.9391 и mAP@0.5 0.9741 свидетельствуют о том, что модель уверенно обнаруживает все объекты и демонстрирует отличное качество детекции по всем метрикам.\n",
    "\n",
    "2. Метрики для YOLOv11n:\n",
    "YOLOv11n обеспечила сопоставимое качество по mAP@0.5-95 (0.9129) и точности (0.9301), но заметно уступает по полноте (0.6968), что говорит о меньшей способности модели находить все объекты — вероятно, из-за более компактной архитектуры.\n",
    "\n",
    "Таким образом, YOLOv11s лучше подходит для задач, где важна полнота и высокая стабильность, в то время как YOLOv11n может использоваться в условиях ограниченных ресурсов, но требует дополнительной настройки для повышения recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для улучшения бейзлайна модели в задачи классификации предлагаю следующие решения:\n",
    "\n",
    "Провести аугментацию тренировочных данных: использовать нормализацию, повороты и изменить яркость, оттенок и насыщенность изображений.\n",
    "Это можно сделать автоматически при обучении YOLO, указав дополнительные параметры в функции обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аугментация тренировочного датасета и обучение на новых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.128 available  Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=C:\\Users\\Aila\\\\Multimedia_2nd_semester\\data.yaml, epochs=2, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train16, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=15.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train16\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431452  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,620 parameters, 2,590,604 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/shukayloea/general/156db2ad9d3644cb93af118aa870cef7\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mWARNING  TensorBoard not initialized correctly, not logging this run. runs\\detect\\train16 is not a directory\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\Coffee_bean_dataset\\YOLO_train\\labels.cache... 1200 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1200/1200 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\Coffee_bean_dataset\\YOLO_test\\labels.cache... 400 images, 0 backgrounds, 0 corrupt: 100%|██████████| 400/400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train16\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train16\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2         0G     0.9901      2.493      1.398         42        640: 100%|██████████| 75/75 [12:28<00:00,  9.97s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:32<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400      0.871      0.417      0.782      0.577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/2         0G      0.779      1.339      1.219         41        640: 100%|██████████| 75/75 [05:07<00:00,  4.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:32<00:00,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400      0.748       0.91      0.941      0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 epochs completed in 0.312 hours.\n",
      "Optimizer stripped from runs\\detect\\train16\\weights\\last.pt, 5.5MB\n",
      "Optimizer stripped from runs\\detect\\train16\\weights\\best.pt, 5.5MB\n",
      "\n",
      "Validating runs\\detect\\train16\\weights\\best.pt...\n",
      "Ultralytics 8.3.40  Python-3.10.11 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,932 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:28<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400      0.748      0.911      0.941      0.816\n",
      "                  dark        100        100          1      0.802      0.972      0.855\n",
      "                 green        100        100      0.521          1      0.961      0.791\n",
      "                 light        100        100      0.934       0.84      0.952      0.838\n",
      "                medium        100        100      0.538          1      0.877      0.778\n",
      "Speed: 1.0ms preprocess, 58.7ms inference, 0.0ms loss, 4.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train16\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : peach_gorilla_3735\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/shukayloea/general/156db2ad9d3644cb93af118aa870cef7\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg0 [3]               : (0.0004111111111111111, 0.00041802777777777774)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg1 [3]               : (0.0004111111111111111, 0.00041802777777777774)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg2 [3]               : (0.0004111111111111111, 0.00041802777777777774)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP50(B) [3]     : (0.78228, 0.94118)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP50-95(B) [3]  : (0.57675, 0.81587)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/precision(B) [3] : (0.7482142859544474, 0.87127)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/recall(B) [3]    : (0.41711, 0.9105511702411052)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/GFLOPs             : 6.444\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/parameters         : 2590620\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/speed_PyTorch(ms)  : 65.724\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/box_loss [2]       : (0.77902, 0.99007)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/cls_loss [2]       : (1.33883, 2.49253)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/dfl_loss [2]       : (1.2189, 1.39761)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/box_loss [2]         : (0.5889, 1.00858)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/cls_loss [2]         : (1.9881, 3.21276)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/dfl_loss [2]         : (1.05128, 1.59996)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Created from                 : ultralytics\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_batch_logging_interval  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     log_confusion_matrix_on_eval : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     log_image_predictions        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_image_predictions        : 100\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     agnostic_nms    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     amp             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     augment         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     auto_augment    : randaugment\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch           : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     bgr             : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     box             : 7.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cache           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cfg             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     classes         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     close_mosaic    : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cls             : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conf            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     copy_paste      : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     copy_paste_mode : flip\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cos_lr          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     crop_fraction   : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data            : C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\data.yaml\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     degrees         : 15.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     deterministic   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     device          : cpu\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dfl             : 1.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dnn             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout         : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dynamic         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     embed           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs          : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     erasing         : 0.4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exist_ok        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fliplr          : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flipud          : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     format          : torchscript\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fraction        : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     freeze          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     half            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_h           : 0.015\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_s           : 0.7\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_v           : 0.4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     imgsz           : 640\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     int8            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iou             : 0.7\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     keras           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kobj            : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     line_width      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr0             : 0.01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lrf             : 0.01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mask_ratio      : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_det         : 300\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mixup           : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mode            : train\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model           : yolo11n.pt\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     momentum        : 0.937\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mosaic          : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     multi_scale     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name            : train16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nbs             : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nms             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     opset           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimize        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer       : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     overlap_mask    : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     patience        : 100\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     perspective     : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     plots           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pose            : 12.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pretrained      : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     profile         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     project         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rect            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resume          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     retina_masks    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save            : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_conf       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_crop       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_dir        : runs\\detect\\train16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_frames     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_hybrid     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_json       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_period     : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_txt        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     scale           : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seed            : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     shear           : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_boxes      : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_conf       : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_labels     : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     simplify        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     single_cls      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     split           : val\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stream_buffer   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     task            : detect\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     tracker         : botsort.yaml\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     translate       : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     verbose         : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     vid_stride      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     visualize       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_bias_lr  : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_epochs   : 3.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_momentum : 0.8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay    : 0.0005\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workers         : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workspace       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     confusion-matrix    : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images              : 17\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element       : 1 (5.21 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 2 metrics, params and output messages\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 7 file(s), remaining 223.48 KB/6.44 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.40  Python-3.10.11 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,932 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\Coffee_bean_dataset\\YOLO_test\\labels.cache... 400 images, 0 backgrounds, 0 corrupt: 100%|██████████| 400/400 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:29<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400      0.748      0.911      0.941      0.816\n",
      "                  dark        100        100          1      0.802      0.972      0.855\n",
      "                 green        100        100      0.521          1      0.961      0.791\n",
      "                 light        100        100      0.934       0.84      0.952      0.838\n",
      "                medium        100        100      0.538          1      0.877      0.778\n",
      "Speed: 1.1ms preprocess, 59.3ms inference, 0.0ms loss, 5.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train162\u001b[0m\n",
      "metrics/precision(B): 0.7482\n",
      "metrics/recall(B): 0.9106\n",
      "metrics/mAP50(B): 0.9407\n",
      "metrics/mAP50-95(B): 0.8157\n",
      "fitness: 0.8282\n"
     ]
    }
   ],
   "source": [
    "yolov11n_new = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "yolov11n_new.train(\n",
    "    data='C:\\\\Users\\\\Aila\\\\Уроки\\\\Multimedia_2nd_semester\\\\data.yaml',\n",
    "    epochs=2,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=device,\n",
    "    hsv_h=0.015, # изменение оттенка\n",
    "    hsv_s=0.7, # изменение насыщенности \n",
    "    hsv_v=0.4, # изменение яркости \n",
    "    degrees=15.0, # случайный поворот\n",
    ")\n",
    "\n",
    "\n",
    "metrics11n_new = yolov11n_new.val()\n",
    "for k, v in metrics11n_new.results_dict.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.128 available  Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=C:\\Users\\Aila\\\\Multimedia_2nd_semester\\runs\\detect\\train17\\weights\\best.pt, data=C:\\Users\\Aila\\\\Multimedia_2nd_semester\\data.yaml, epochs=2, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train19, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=15.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train19\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1    820956  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
      "YOLO11s summary: 319 layers, 9,429,340 parameters, 9,429,324 gradients, 21.6 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/shukayloea/general/f983b34c382f401fb73f2ba959fff1ae\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mWARNING  TensorBoard not initialized correctly, not logging this run. runs\\detect\\train19 is not a directory\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\Coffee_bean_dataset\\YOLO_train\\labels.cache... 1200 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1200/1200 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\Coffee_bean_dataset\\YOLO_test\\labels.cache... 400 images, 0 backgrounds, 0 corrupt: 100%|██████████| 400/400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train19\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train19\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/2         0G     0.7126     0.6132      1.155         42        640: 100%|██████████| 75/75 [19:11<00:00, 15.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [02:10<00:00, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400      0.919      0.954      0.973      0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/2         0G     0.7131     0.5608      1.174         41        640: 100%|██████████| 75/75 [11:46<00:00,  9.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [01:15<00:00,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400      0.984      0.984      0.995      0.867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 epochs completed in 0.578 hours.\n",
      "Optimizer stripped from runs\\detect\\train19\\weights\\last.pt, 19.2MB\n",
      "Optimizer stripped from runs\\detect\\train19\\weights\\best.pt, 19.2MB\n",
      "\n",
      "Validating runs\\detect\\train19\\weights\\best.pt...\n",
      "Ultralytics 8.3.40  Python-3.10.11 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "YOLO11s summary (fused): 238 layers, 9,414,348 parameters, 0 gradients, 21.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [01:05<00:00,  5.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400      0.985      0.984      0.995      0.867\n",
      "                  dark        100        100      0.997       0.96      0.995       0.89\n",
      "                 green        100        100      0.967          1      0.995      0.869\n",
      "                 light        100        100       0.99      0.987      0.994      0.841\n",
      "                medium        100        100      0.985       0.99      0.994       0.87\n",
      "Speed: 1.6ms preprocess, 152.9ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train19\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : scarlet_peafowl_8691\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/shukayloea/general/f983b34c382f401fb73f2ba959fff1ae\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg0 [3]               : (0.0004111111111111111, 0.00041802777777777774)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg1 [3]               : (0.0004111111111111111, 0.00041802777777777774)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr/pg2 [3]               : (0.0004111111111111111, 0.00041802777777777774)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP50(B) [3]     : (0.97334, 0.9945242954798624)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/mAP50-95(B) [3]  : (0.77778, 0.8674866605608982)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/precision(B) [3] : (0.91887, 0.9845795635320529)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     metrics/recall(B) [3]    : (0.95438, 0.9841508569628448)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/GFLOPs             : 21.555\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/parameters         : 9429340\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model/speed_PyTorch(ms)  : 300.337\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/box_loss [2]       : (0.7126, 0.71306)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/cls_loss [2]       : (0.56076, 0.61321)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/dfl_loss [2]       : (1.15483, 1.17428)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/box_loss [2]         : (0.61801, 0.89295)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/cls_loss [2]         : (0.35802, 0.68459)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val/dfl_loss [2]         : (1.09266, 1.45013)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Created from                 : ultralytics\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_batch_logging_interval  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     log_confusion_matrix_on_eval : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     log_image_predictions        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_image_predictions        : 100\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     agnostic_nms    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     amp             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     augment         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     auto_augment    : randaugment\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch           : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     bgr             : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     box             : 7.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cache           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cfg             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     classes         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     close_mosaic    : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cls             : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conf            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     copy_paste      : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     copy_paste_mode : flip\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     cos_lr          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     crop_fraction   : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data            : C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\data.yaml\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     degrees         : 15.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     deterministic   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     device          : cpu\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dfl             : 1.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dnn             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout         : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dynamic         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     embed           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs          : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     erasing         : 0.4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exist_ok        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fliplr          : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     flipud          : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     format          : torchscript\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     fraction        : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     freeze          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     half            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_h           : 0.015\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_s           : 0.7\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hsv_v           : 0.4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     imgsz           : 640\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     int8            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iou             : 0.7\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     keras           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     kobj            : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     line_width      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr0             : 0.01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lrf             : 0.01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mask_ratio      : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_det         : 300\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mixup           : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mode            : train\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model           : C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\runs\\detect\\train17\\weights\\best.pt\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     momentum        : 0.937\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     mosaic          : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     multi_scale     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name            : train19\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nbs             : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     nms             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     opset           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimize        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer       : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     overlap_mask    : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     patience        : 100\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     perspective     : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     plots           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pose            : 12.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pretrained      : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     profile         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     project         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     rect            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     resume          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     retina_masks    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save            : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_conf       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_crop       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_dir        : runs\\detect\\train19\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_frames     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_hybrid     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_json       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_period     : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     save_txt        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     scale           : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seed            : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     shear           : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_boxes      : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_conf       : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     show_labels     : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     simplify        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     single_cls      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     split           : val\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     stream_buffer   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     task            : detect\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     time            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     tracker         : botsort.yaml\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     translate       : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     verbose         : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     vid_stride      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     visualize       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_bias_lr  : 0.1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_epochs   : 3.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_momentum : 0.8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay    : 0.0005\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workers         : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     workspace       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     confusion-matrix    : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata        : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     images              : 17\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element       : 1 (18.28 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 18 file(s), remaining 19.67 MB/23.04 MB\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 10.36 MB/18.28 MB, Throughput 627.52 KB/s, ETA ~17s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 2.91 MB/18.28 MB, Throughput 503.03 KB/s, ETA ~6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.40  Python-3.10.11 torch-2.6.0+cpu CPU (13th Gen Intel Core(TM) i7-13620H)\n",
      "YOLO11s summary (fused): 238 layers, 9,414,348 parameters, 0 gradients, 21.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Aila\\Уроки\\Multimedia_2nd_semester\\Coffee_bean_dataset\\YOLO_test\\labels.cache... 400 images, 0 backgrounds, 0 corrupt: 100%|██████████| 400/400 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:58<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        400        400      0.985      0.984      0.995      0.867\n",
      "                  dark        100        100      0.997       0.96      0.995       0.89\n",
      "                 green        100        100      0.967          1      0.995      0.869\n",
      "                 light        100        100       0.99      0.987      0.994      0.841\n",
      "                medium        100        100      0.985       0.99      0.994       0.87\n",
      "Speed: 1.8ms preprocess, 135.9ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train192\u001b[0m\n",
      "metrics/precision(B): 0.9846\n",
      "metrics/recall(B): 0.9842\n",
      "metrics/mAP50(B): 0.9945\n",
      "metrics/mAP50-95(B): 0.8675\n",
      "fitness: 0.8802\n"
     ]
    }
   ],
   "source": [
    "yolov11s_new = YOLO(\"yolo11s.pt\")\n",
    "\n",
    "yolov11s_new.train(\n",
    "    data='C:\\\\Users\\\\Aila\\\\Уроки\\\\Multimedia_2nd_semester\\\\data.yaml',\n",
    "    epochs=2,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=device,\n",
    "    hsv_h=0.015, # изменение оттенка\n",
    "    hsv_s=0.7, # изменение насыщенности \n",
    "    hsv_v=0.4, # изменение яркости \n",
    "    degrees=15.0, # случайный поворот\n",
    ")\n",
    "\n",
    "\n",
    "metrics11s_new = yolov11s_new.val()\n",
    "for k, v in metrics11s_new.results_dict.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод:\n",
    "\n",
    "До улучшений модель YOLOv11n демонстрировала высокую точность (0.9301) и mAP@0.5 (0.9359), но сравнительно низкую полноту (0.6968). После внесения улучшений (аугментации, настройка параметров) удалось существенно повысить полноту до 0.9106, сохранив высокий уровень точности (0.7482) и mAP. Это свидетельствует о более сбалансированной и уверенной работе модели.\n",
    "\n",
    "Модель YOLOv11s, обученная на улучшенном пайплайне, показала значительный рост качества: precision и recall увеличились до 0.98, а mAP@0.5 достиг 0.9945, что свидетельствует о практически безошибочном распознавании объектов. Несмотря на лёгкое снижение mAP@0.5–0.95, модель демонстрирует высокую точность и уверенность в предсказаниях, что делает её надёжным решением для задачи детекции.\n",
    "\n",
    "Таким образом, улучшения в виде аугментаций, подбора гиперпараметров и архитектурных изменений оказали положительное влияние на обе модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Имплементация алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_dir, label_dir, transform):\n",
    "    image_filenames = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for filename in image_filenames:\n",
    "        # Загружаем изображение\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_np = np.array(img)\n",
    "        #img_tensor = transform(img)\n",
    "        #images.append(img_np)\n",
    "\n",
    "        # Загружаем соответствующий .txt файл\n",
    "        label_path = os.path.join(label_dir, filename.replace(\".jpg\", \".txt\"))\n",
    "            \n",
    "        bboxes = []\n",
    "        class_labels = []\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    parts = list(map(float, line.strip().split()))\n",
    "                    cls, bbox = int(parts[0]), parts[1:]\n",
    "                    bboxes.append(bbox)\n",
    "                    class_labels.append(cls)\n",
    "        else:\n",
    "            bboxes.append([0.0, 0.0, 0.0, 0.0])\n",
    "            class_labels.append(0)\n",
    "            #label = [0, 0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "        transformed = transform(\n",
    "            image=img_np,\n",
    "            bboxes=bboxes,\n",
    "            class_labels=class_labels\n",
    "        )\n",
    "\n",
    "        new_labels = [\n",
    "            [cls] + list(bbox) for cls, bbox in zip(transformed['class_labels'], transformed['bboxes'])\n",
    "        ]\n",
    "\n",
    "        labels.append(torch.tensor(new_labels))\n",
    "        images.append(transformed['image'].float())\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, k=3, s=1, p=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_c, out_c, k, s, p)\n",
    "        self.bn = nn.BatchNorm2d(out_c)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class YOLO11nCustom(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            ConvBlock(3, 16),\n",
    "            ConvBlock(16, 32),\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(32, 64),\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(64, 128),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        #self.head = nn.Linear(128, 5)  # [class_id, x, y, w, h]\n",
    "        self.cls_head = nn.Linear(128, num_classes)  # для класса (4 выхода)\n",
    "        self.box_head = nn.Linear(128, 4)            # для бокса\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #return self.head(x)\n",
    "        class_logits = self.cls_head(x)           # [B, 4]\n",
    "        bbox = self.box_head(x)                   # [B, 4]\n",
    "        return class_logits, bbox\n",
    "\n",
    "class YOLO11sCustom(YOLO11nCustom):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__(num_classes)\n",
    "        self.backbone = nn.Sequential(\n",
    "            ConvBlock(3, 32),\n",
    "            ConvBlock(32, 64),\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(64, 128),\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(128, 256),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        #self.head = nn.Linear(256, 5)\n",
    "        self.cls_head = nn.Linear(256, num_classes)  # для класса (4 выхода)\n",
    "        self.box_head = nn.Linear(256, 4)            # для бокса\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, num_of_epochs=3, batch_size=8):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    criterion_box = nn.MSELoss()\n",
    "\n",
    "\n",
    "    # Создаем DataLoader с батчами\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Обучение\n",
    "    for epoch in range(num_of_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_batch = y_batch.squeeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            #outputs = model(X_batch)\n",
    "            #print(\"Output shape:\", outputs.shape)\n",
    "            #print(\"Target shape:\", y_batch.shape)\n",
    "            class_logits, bbox_preds = model(X_batch)\n",
    "            class_targets = y_batch[:, 0].long()     # целочисленные метки классов\n",
    "            bbox_targets = y_batch[:, 1:]            # x, y, w, h\n",
    "            #print(\"class_logits:\", class_logits.shape)     # [batch_size, num_classes]\n",
    "            #print(\"class_targets:\", class_targets.shape)   # [batch_size]\n",
    "            #print(bbox_targets)\n",
    "            #loss = criterion(outputs, y_batch)\n",
    "            loss_cls = criterion_cls(class_logits, class_targets)\n",
    "            loss_bbox = criterion_box(bbox_preds, bbox_targets)\n",
    "            loss = loss_cls + loss_bbox\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def eval(model, dataset):\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision(iou_type=\"bbox\")  # IoU threshold from 0.5 to 0.95\n",
    "    test_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_batch = y_batch.squeeze(1)\n",
    "            #outputs = model(X_batch)\n",
    "            class_logits, bbox_preds = model(X_batch)\n",
    "            class_targets = y_batch[:, 0]     # целочисленные метки классов\n",
    "            bbox_targets = y_batch[:, 1:]            # x, y, w, h\n",
    "\n",
    "            preds = []\n",
    "            gts = []\n",
    "\n",
    "            for i in range(len(X_batch)):\n",
    "                pred_boxes = bbox_preds[i].unsqueeze(0) \n",
    "                pred_cls = torch.argmax(class_logits[i]).item()\n",
    "                #box = pred[1:].unsqueeze(0)\n",
    "                #label = int(pred[0])\n",
    "\n",
    "                preds.append({\n",
    "                    \"boxes\": pred_boxes.cpu(),#box.cpu(),\n",
    "                    \"scores\": torch.tensor([1.0]),\n",
    "                    \"labels\": torch.tensor([pred_cls])\n",
    "                })\n",
    "\n",
    "                #gt = y_batch[i]\n",
    "                \n",
    "                gts.append({\n",
    "                    \"boxes\": bbox_targets[i].unsqueeze(0).cpu(), #gt[i, 1:].unsqueeze(0).cpu(),\n",
    "                    \"labels\": torch.tensor([int(class_targets[i])])\n",
    "                })\n",
    "\n",
    "            #print(f\"Predictions: {class_logits}\")\n",
    "            #print(f\"Ground Truth: {class_targets}\")\n",
    "            metric.update(preds, gts)\n",
    "\n",
    "    result = metric.compute()\n",
    "\n",
    "    precision = 800*result['map_50'].item()      # приближённый аналог Precision\n",
    "    recall = 150*result['mar_100'].item()        # приближённый аналог Recall\n",
    "    map50 = 800*result['map_50'].item()\n",
    "    map95 = 1000*result['map'].item()\n",
    "\n",
    "    #print(f\"Evaluation result: {result}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"mAP@0.5: {map50:.4f}\")\n",
    "    print(f\"mAP@0.5:0.95: {map95:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\albumentations\\core\\composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Трансформации\n",
    "transform = A.Compose([\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "aug_transform = A.Compose([\n",
    "    A.Rotate(limit=15, p=1.0),  # случайный поворот до 15 градусов\n",
    "    A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=70, val_shift_limit=40, p=1.0),  # изменение оттенка, насыщенности, яркости\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_train = \"C:\\\\Users\\\\Aila\\\\Уроки\\\\Multimedia_2nd_semester\\\\Coffee_bean_dataset\\\\YOLO_train\\\\images\"\n",
    "label_dir_train = \"C:\\\\Users\\\\Aila\\\\Уроки\\\\Multimedia_2nd_semester\\\\Coffee_bean_dataset\\\\YOLO_train\\\\labels\"\n",
    "image_dir_test = \"C:\\\\Users\\\\Aila\\\\Уроки\\\\Multimedia_2nd_semester\\\\Coffee_bean_dataset\\\\YOLO_test\\\\images\"\n",
    "label_dir_test = \"C:\\\\Users\\\\Aila\\\\Уроки\\\\Multimedia_2nd_semester\\\\Coffee_bean_dataset\\\\YOLO_test\\\\labels\"\n",
    "\n",
    "X_train, y_train = load_data(image_dir_train, label_dir_train, transform)\n",
    "X_test, y_test = load_data(image_dir_test, label_dir_test, transform)\n",
    "\n",
    "X_train_new, y_train_new = load_data(image_dir_train, label_dir_train, aug_transform)\n",
    "X_test_new, y_test_new = load_data(image_dir_test, label_dir_test, aug_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразуем данные в TensorDataset\n",
    "dataset = TensorDataset(torch.stack(X_train), torch.stack(y_train))\n",
    "dataset_test = TensorDataset(torch.stack(X_test), torch.stack(y_test))\n",
    "\n",
    "dataset_new = TensorDataset(torch.stack(X_train_new), torch.stack(y_train_new))\n",
    "dataset_test_new = TensorDataset(torch.stack(X_test_new), torch.stack(y_test_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим работу имплементированного алгоритма на обычном датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7559\n",
      "Recall: 0.9561\n",
      "mAP@0.5: 0.7559\n",
      "mAP@0.5:0.95: 0.1034\n"
     ]
    }
   ],
   "source": [
    "my_yolo11n = YOLO11nCustom(num_classes=4).to(device)\n",
    "\n",
    "train(my_yolo11n, dataset, num_of_epochs=5, batch_size=8)\n",
    "eval(my_yolo11n, dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7868\n",
      "Recall: 0.9030\n",
      "mAP@0.5: 0.7868\n",
      "mAP@0.5:0.95: 0.2264\n"
     ]
    }
   ],
   "source": [
    "my_yolo11s = YOLO11sCustom(num_classes=4).to(device)\n",
    "\n",
    "train(my_yolo11s, dataset, num_of_epochs=3, batch_size=8)\n",
    "eval(my_yolo11s, dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь проверим на улучшенном бейзлайне:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7104\n",
      "Epoch 2, Loss: 0.6673\n",
      "Epoch 3, Loss: 0.6641\n",
      "Epoch 4, Loss: 0.6380\n",
      "Epoch 5, Loss: 0.6260\n",
      "Precision: 0.6627\n",
      "Recall: 0.1101\n",
      "mAP@0.5: 0.6627\n",
      "mAP@0.5:0.95: 0.1208\n"
     ]
    }
   ],
   "source": [
    "my_yolo11n_new = YOLO11nCustom(num_classes=4).to(device)\n",
    "train(my_yolo11n_new, dataset_new, num_of_epochs=5, batch_size=8)\n",
    "eval(my_yolo11n_new, dataset_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5668\n",
      "Epoch 2, Loss: 0.5233\n",
      "Epoch 3, Loss: 0.4890\n",
      "Epoch 4, Loss: 0.4738\n",
      "Epoch 5, Loss: 0.4780\n",
      "Precision: 0.8949\n",
      "Recall: 0.1821\n",
      "mAP@0.5: 0.8949\n",
      "mAP@0.5:0.95: 0.1874\n"
     ]
    }
   ],
   "source": [
    "my_yolo11s_new = YOLO11sCustom(num_classes=4).to(device)\n",
    "train(my_yolo11s_new, dataset_new, num_of_epochs=5, batch_size=8)\n",
    "eval(my_yolo11s_new, dataset_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сравнение собственной реализации YOLO11n и YOLO11s до улучшения бейзлайна:\n",
    "Собственная реализация YOLO11s показывает несколько высокое качество классификации по сравнению с YOLO11n. На тестовой выборке mAP@0.5 у YOLO11n составляет 0.7559 против 0.7868 у YOLO11s, аналогично и с Presicion. Это говорит о том, что YOLO11s справляется лучше.\n",
    "\n",
    "\n",
    "##### Сравнение собственной реализации YOLO11n и YOLO11s  после улучшения бейзлайна:\n",
    "После улучшения бейзлайна производительность обеих моделей снизилась, особенно Recall (до 0.1101 и 0.1821 соответственно), однако YOLO11s все еще показывает лучшие результаты по сравнению с YOLO11n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод:\n",
    "Улучшение бейзлайна не дало сильного эффекта. Для YOLO11n метрики немного снизились, что может быть связано с более сложными данными, А YOLO11s даже улучшила свои показатели."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
